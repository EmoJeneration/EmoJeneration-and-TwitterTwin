{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff69ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isabellasri/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import PCA\n",
    "stopwords_list = stopwords.words('english')\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stopwords_list = [stemmer.stem(word) for word in stopwords_list]\n",
    "\n",
    "#GRADIO\n",
    "import gradio as gr\n",
    "\n",
    "#BOOSTED DECISION TREE\n",
    "import sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60687765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'/Users/isabellasri/Desktop/emoji_data/DS/group1.csv')\n",
    "dataset2 = pd.read_csv(r'/Users/isabellasri/Desktop/emoji_data/DS/group2.csv')\n",
    "dataset3 = pd.read_csv(r'/Users/isabellasri/Desktop/emoji_data/DS/group3.csv')\n",
    "dataset4 = pd.read_csv(r'//Users/isabellasri/Desktop/emoji_data/DS/group4.csv')\n",
    "dataset5 = pd.read_csv(r'/Users/isabellasri/Desktop/emoji_data/DS/group5.csv')\n",
    "\n",
    "dataset = pd.concat([dataset, dataset2, dataset3, dataset4, dataset5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d99a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING SPECIAL CHARACTERS FROM TWEETS\n",
    "special_characters = ['~', ':', \"'\", '+', '[', '\\\\', '@', '^',\n",
    "                      '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '>', ';', '#', '$', ')','!','?', '/', '‚Äô', '‚Äú', '‚Äù', \"‚Ä¶\"]\n",
    "\n",
    "myoldemolist = dataset.loc[:, \"clean\"].tolist()\n",
    "\n",
    "def replace_special(myemolist, myspeciallist):\n",
    "    for i in myspeciallist:\n",
    "        for j in range(len(myemolist)):\n",
    "            myemolist[j] = myemolist[j].replace(i, \"\")\n",
    "    return myemolist\n",
    "\n",
    "my_new_clean = replace_special(myoldemolist, special_characters)\n",
    "\n",
    "dataset[\"myNewClean\"] = my_new_clean\n",
    "\n",
    "# before i dropped 'clean', but ill keep it in by deleting it from the line below\n",
    "\n",
    "dataset = dataset.drop(columns = ['emo', 'handle', 'created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5440aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing exclamations and question marks from this list\n",
    "#Accuracy: 0.7358334319176624"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REORGANIZING AGE COLUMN\n",
    "age_list = list(dataset['age'])\n",
    "for i in range(len(age_list)):\n",
    "    if age_list[i] == 3:\n",
    "        age_list[i] = 2\n",
    "    if age_list[i] == 5:\n",
    "        age_list[i] = 3\n",
    "    if age_list[i] == 4:\n",
    "        age_list[i] = 3\n",
    "\n",
    "dataset['age'] = age_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING STOPWORDS FROM TWEETS\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "dataset['myNewClean'] = dataset['myNewClean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85631caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized, separated the emojis\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "t = TweetTokenizer()\n",
    "dataset['tokenized'] = dataset.apply(lambda x: t.tokenize(x['myNewClean']), axis=1)\n",
    "\n",
    "lang = pd.read_csv(r'/Users/isabellasri/Desktop/emoji_data/DS/lang.csv')\n",
    "lang_list = lang['0'].to_list()\n",
    "dataset['lang'] = lang_list\n",
    "\n",
    "lang_list = (\"en\", \"other\")\n",
    "dataset = dataset[dataset.lang.isin(lang_list)]\n",
    "\n",
    "word_list = dataset['tokenized'].tolist()\n",
    "\n",
    "def word_dict(word_list):\n",
    "    word_dict = {}\n",
    "    for tweet in word_list:\n",
    "        for word in tweet:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = 1\n",
    "            else:\n",
    "                word_dict[word] += 1\n",
    "    return(word_dict)\n",
    "\n",
    "word_list = word_dict(word_list)\n",
    "\n",
    "word_df = pd.DataFrame(word_list.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb38034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF IDF\n",
    "\n",
    "tokenized_tweets = list(dataset['tokenized'])\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, max_features = 1000)  \n",
    "\n",
    "tf_tweets = tfidf.fit_transform(tokenized_tweets)\n",
    "tfidf_array = tf_tweets.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "NUM_COMPONENTS = 1000\n",
    "pca = PCA(NUM_COMPONENTS)\n",
    "reduced = pca.fit_transform(tfidf_array)\n",
    "\n",
    "variance_explained = np.cumsum(pca.explained_variance_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "plt.plot(range(NUM_COMPONENTS),variance_explained, color='r')\n",
    "ax.grid(True)\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "#It takes around 300 components to explain 60% of variance. While 300 components post that expl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f5db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###GRADIENT BOOSTED DECISION TREE\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_array, dataset['age'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Build the model\n",
    "gbdt = GradientBoostingClassifier(n_estimators=500, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "\n",
    "# Train the model on the training set\n",
    "print('Begin gbdt' + '-' * 50)\n",
    "gbdt.fit(X_train, y_train)\n",
    "print('End gbdt' + '-' * 50)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict the classes of the test set\n",
    "print('Begin prediction' + '-' * 50)\n",
    "y_pred = gbdt.predict(X_test)\n",
    "print('End prediction' + '-' * 50)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "pickle.dump(gbdt, open('gbdt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_gbdt = pickle.load(open('gbdt.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e47a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd8eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_tfidf(tfidf_matrix, expected_size):\n",
    "    # Get the current size of the TF-IDF matrix\n",
    "    current_size = tfidf_matrix.shape[1]\n",
    "\n",
    "    # If the current size is less than the expected size, add zero columns to the right of the matrix\n",
    "    if current_size < expected_size:\n",
    "        zero_columns = np.zeros((tfidf_matrix.shape[0], expected_size - current_size))\n",
    "        padded_tfidf_matrix = np.hstack((tfidf_matrix, zero_columns))\n",
    "\n",
    "    # If the current size is greater than or equal to the expected size, truncate the matrix to the expected size\n",
    "    else:\n",
    "        padded_tfidf_matrix = tfidf_matrix[:, :expected_size]\n",
    "\n",
    "    return padded_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c387db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIO - prediction 2\n",
    "\n",
    "def predict(text):\n",
    "    try:\n",
    "        # Preprocess the input text\n",
    "        text = text.lower()\n",
    "        clean_text = replace_special([text], special_characters)[0]\n",
    "        print(clean_text)\n",
    "        tokens = word_tokenize(clean_text)\n",
    "        filtered_text = [word for word in tokens if not word in stopwords.words()]\n",
    "        \n",
    "        # Compute the TF-IDF representation of the preprocessed text\n",
    "        tfidf2 = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, max_features=1000)  \n",
    "        tf_str = tfidf2.fit_transform([filtered_text])\n",
    "        tfidf2_array = tf_str.toarray()\n",
    "\n",
    "        # Apply scaling and padding to the TF-IDF array\n",
    "        preprocessed = scaler.fit_transform(tfidf2_array)\n",
    "        padded_tfidf_text = reshape_tfidf(preprocessed, 1000)\n",
    "\n",
    "        # Make a prediction using the GBDT model\n",
    "        prediction = pickled_gbdt.predict(padded_tfidf_text)\n",
    "\n",
    "        return prediction[0]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions raised by the function calls\n",
    "        error_message = f\"An error occurred during prediction: {str(e)}\"\n",
    "        return error_message\n",
    "print(predict('thanks dad lolüò≠üò≠ .@mythical '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a4e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIO - interface\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.inputs.Textbox(label=\"Input text\"),\n",
    "    outputs=gr.outputs.Label(label=\"Age Group\"))\n",
    "\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F1 SCORE\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# define the positive class\n",
    "pos_label = 1\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, pos_label=pos_label, average='weighted')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
