{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff69ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing the necessary packages\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.datasets import make_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1a1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read in the desired dataset\n",
    "dataset = pd.read_csv(filepath)\n",
    "dataset['age'] = dataset['age'].astype(int)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d99a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Removing the special characters from the tweets.\n",
    "# We wrote our own function to retain the emojis\n",
    "special_characters = ['~', ':', \"'\", '+', '[', '\\\\', '@', '^',\n",
    "                      '{', '%', '(', '-', '\"', '*', '|', ',', '&', '<', '`', '}', '.', '_', '=', ']', '>', ';', '#', '$', ')','!','?', '/', '’', '“', '”', \"…\"]\n",
    "\n",
    "myoldemolist = dataset.loc[:, \"clean\"].tolist()\n",
    "\n",
    "def replace_special(myemolist, myspeciallist):\n",
    "    for i in myspeciallist:\n",
    "        for j in range(len(myemolist)):\n",
    "            myemolist[j] = myemolist[j].replace(i, \"\")\n",
    "    return myemolist\n",
    "\n",
    "my_new_clean = replace_special(myoldemolist, special_characters)\n",
    "\n",
    "# Updating our dataset data frame to include the \n",
    "\n",
    "dataset[\"myNewClean\"] = my_new_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ab438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS CELL IF YOU WANT TO CREATE A MODEL THAT DOESN'T CONSIDER EMOJIS\n",
    "# Define a function to remove emojis using a regular expression\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           u\"\\U0001FB00-\\U0001FBFF\"  # Symbols and Pictographs Extended-B\n",
    "                           u\"\\U0001F004-\\U0001F0CF\"  # Mahjong Tiles\n",
    "                           u\"\\U0001F170-\\U0001F251\"  # Enclosed Ideographic Supplement\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the removal function to the 'text' column\n",
    "dataset['myNewClean'] = dataset['myNewClean'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING STOPWORDS FROM TWEETS\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "dataset['myNewClean'] = dataset['myNewClean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b924fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized, separated the emojis\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "t = TweetTokenizer()\n",
    "dataset['tokenized'] = dataset.apply(lambda x: t.tokenize(x['myNewClean']), axis=1)\n",
    "\n",
    "word_list = dataset['tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb38034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a TF-IDF vectorizer using the dataset\n",
    "\n",
    "tokenized_tweets = list(dataset['tokenized'])\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False, max_features = 1000)  \n",
    "\n",
    "tf_tweets = tfidf.fit_transform(tokenized_tweets)\n",
    "tfidf_array = tf_tweets.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing xgboost to use gradient decision trees\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9f5db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###GRADIENT BOOSTED DECISION TREE\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "#age-1\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_array,dataset['age']-1, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e49844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model, change parameters as needed\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='multi:softmax',  # For multi-class classification\n",
    "    num_class=len(np.unique(y_train)),\n",
    "    max_depth=6,                # Maximum depth of each tree\n",
    "    n_estimators=500,           # Number of boosting rounds\n",
    "    learning_rate=0.1,          # Learning rate (step size shrinkage)\n",
    "    subsample=0.8,              # Fraction of samples used for training each tree\n",
    "    colsample_bytree=0.8,       # Fraction of features used for training each tree\n",
    "    random_state=42             # Seed for reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa0797d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33bc54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set using the model and finding the accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = sklearn.metrics.accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating overall precision, recall and F1 scores\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# define the positive class\n",
    "pos_label = 1\n",
    "\n",
    "# calculate precision, recall, and F1 score\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, pos_label=pos_label, average='weighted')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('F1 Score:', f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision = precision_score(y_test, y_pred, average=None)\n",
    "recall = recall_score(y_test, y_pred, average=None)\n",
    "f1 = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "for class_label in range(5):\n",
    "    print(f\"Class {class_label}:\")\n",
    "    print(f\"Precision: {precision[class_label]}\")\n",
    "    print(f\"Recall: {recall[class_label]}\")\n",
    "    print(f\"F1 Score: {f1[class_label]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06806373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing precision, recall, and F1 scores by class\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# List of class labels (e.g., Class 0, Class 1, Class 2)\n",
    "class_labels = [f'Class {i}' for i in range(len(precision))]\n",
    "\n",
    "# Values for precision, recall, and F1 score for each class\n",
    "precision_values = precision\n",
    "recall_values = recall\n",
    "f1_values = f1\n",
    "\n",
    "# Create subplots for precision, recall, and F1 score\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(8, 12))\n",
    "\n",
    "# Plot precision\n",
    "axes[0].bar(class_labels, precision_values, color='b', alpha=0.7)\n",
    "axes[0].set_ylabel('Precision')\n",
    "axes[0].set_title('Precision for Each Class')\n",
    "\n",
    "# Plot recall\n",
    "axes[1].bar(class_labels, recall_values, color='g', alpha=0.7)\n",
    "axes[1].set_ylabel('Recall')\n",
    "axes[1].set_title('Recall for Each Class')\n",
    "\n",
    "# Plot F1 score\n",
    "axes[2].bar(class_labels, f1_values, color='r', alpha=0.7)\n",
    "axes[2].set_ylabel('F1 Score')\n",
    "axes[2].set_title('F1 Score for Each Class')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "plt.savefig()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
